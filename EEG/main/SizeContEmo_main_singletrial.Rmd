---
title: "<center> <h1>***SIZECONTEMO***</h1> </center>"
author: '[Antonio Schettino](https://www.researchgate.net/profile/Antonio_Schettino2 "Antonio Schettino")'
date: '`r Sys.Date()`'
output:
  html_document:
    theme: united
    highlight: tango
    code_folding: hide
    toc: true
    toc_float: true
---

In this project we will investigate the electrophysiological correlates of the combined processing of basic visual properties (i.e., size and contrast) and the emotional content of simple words. 

```{r setup_environment,echo=FALSE,warning=FALSE,message=FALSE}
# setup work environment
# dev.off() # clear plots (if no plots are present, comment it out or it will throw an error)
cat("\014") # clear console
rm(list=ls()) # clear environment
set.seed(9001) # specify seed for RNG and ensure reproducible results (it's over 9000!)

expname <- "SizeContEmo" # experiment name
expphase <- "main" # experiment phase
wd <- paste0("D:/OneDrive - UGent/",expname,"/analysis/EEG/",expphase,"/") # work directory
setwd(wd) # set work directory

# load relevant libraries
library(knitr) # dynamic report generation
library(reshape2) # to reshape the data
library(tidyverse) # install the following packages: ggplot2, tibble, tidyr, readr, purrr, dplyr
library(viridis) # more color maps
library(ggthemes) # more themes for ggplot2
library(Rmisc) # for advanced summary functions
library(akima) # interpolation of irregularly and regularly spaced data
library(scales) # scale functions for visualization
library(mgcv) # general additive (mixed) models and other generalized ridge regression with multiple smoothing parameter estimation
library(grid) # for graphics
library(gridExtra) # extra functions for graphics
library(yarrr) # amazing graphs
library(BayesFactor) # calculate Bayes factors

# setup output
options(width=120,scipen=999,digits=3) # change output width (for better printing), disable scientific notation (default: scipen=0), constrain output to 4 decimals
opts_chunk$set(warning=FALSE,message=FALSE,fig.width=10,fig.height=6) # for each chunk, display the output but not the R code (echo=FALSE), no package warnings (message=FALSE), no package messages (message=FALSE), width and height of all figures
```

<center> <h1>*GRAPHS*</h1> </center>

```{r graph_grand_avg}
grand.avg.allbins <- read.csv(paste0(expphase,"_grand_avg_allbins.csv"),header=TRUE,check.names=FALSE) # load data (check.names=FALSE eliminates the "X" at the beginning of each timepoint)
grand.avg.allbins <- melt(grand.avg.allbins,id.vars="electrode",variable.name="timepoint",value.name="amplitude") # transform to long format
grand.avg.allbins$timepoint <- as.numeric(levels(grand.avg.allbins$timepoint))[grand.avg.allbins$timepoint] # convert time points to number

# plot
ggplot(grand.avg.allbins,aes(timepoint,amplitude)) + # basic plot
  geom_line(aes(color=electrode),size=1) + # one line per electrode
  scale_color_viridis(discrete=TRUE,option="magma",alpha=.4) + # color scale
  guides(color="none") + # no legend
  labs(title="grand average across bins (all electrodes)",x="time (ms)",y=expression(paste("amplitude (",mu,"V)"))) + # title & axes labels
  scale_x_continuous(breaks=seq(-200,1000,100)) + # x-axis: tick marks
  scale_y_reverse(breaks=seq(-3,3,1),limits=c(3,-3)) + # y-axis: tick marks
  geom_vline(xintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # vertical reference line
  geom_hline(yintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # horizontal reference line
  theme_pander(base_size=20,pc="white",lp=c(.8,.8)) # custom theme
```

```{r prepare_topos}
## plot electrode coordinates
electrodeLocs <- read_delim(paste0(wd,"leipzig64_Christopher.locs"), # load electrode locations
                            "\t", # delimiter (tab)
                            col_names=c("chanNo","theta","radius","electrode"), # column names
                            escape_double=FALSE,trim_ws=TRUE)

# convert theta values from degrees to radians
electrodeLocs$radianTheta <- pi/180*electrodeLocs$theta
# calculate Cartesian coordinates
electrodeLocs <- electrodeLocs %>% 
  mutate(x=.$radius*sin(.$radianTheta),
         y=.$radius*cos(.$radianTheta))
electrodeLocs <- electrodeLocs[order(electrodeLocs$electrode),] # sort electrodes in alphabetical orde

# create ggplot theme without axes and background grids
theme_topo <- function(base_size=12) {
  theme_bw(base_size=base_size) %+replace%
    theme(rect=element_blank(),
          line= element_blank(),
          axis.text=element_blank(),
          axis.title=element_blank()) }

# function to draw a circle (the head)
circleFun <- function(center=c(0,0),diameter=1,npoints=100) {
  r=diameter/2
  tt <- seq(0,2*pi,length.out=npoints)
  xx <- center[1]+r*cos(tt)
  yy <- center[2]+r*sin(tt)
  return(data.frame(x=xx,y=yy)) }

headShape <- circleFun(c(0,0),1,npoints=100)
nose <- data.frame(x=c(-0.075,0,.075),y=c(.495,.575,.495))

# # plot electrode coordinates on head
# ggplot(headShape,aes(x,y)) +
#   geom_path() +
#   geom_text(data=electrodeLocs,aes(x,y,label=electrode)) +
#   geom_line(data=nose,aes(x,y)) +
#   theme_topo() +
#   coord_equal()

# prepare topography
jet.colors <- colorRampPalette(c("#00007F","blue","#007FFF","cyan","#7FFF7F","yellow","#FF7F00","red","#7F0000")) # jet color map
gridRes <- 268 # number of points for each grid dimension, corresponding to the resolution/smoothness of the interpolation
maskRing <- circleFun(diameter=1.42) # create a circle around the outside of the plotting area to mask the jagged edges of the interpolation
```

```{r load_data_grand_avg_comps}
comps.grand.avg.allbins <- read.csv(paste0(expphase,"_comp_timecourse.csv"),header=TRUE,check.names=FALSE) # load data (check.names=FALSE eliminates the "X" at the beginning of each timepoint)
comps.grand.avg.allbins <- melt(comps.grand.avg.allbins,id.vars=c("component","bin"),variable.name="timepoint",value.name="amplitude") # transform to long format
comps.grand.avg.allbins$timepoint <- as.numeric(levels(comps.grand.avg.allbins$timepoint))[comps.grand.avg.allbins$timepoint] # convert time points to number
levels(comps.grand.avg.allbins$bin) <- c("all","negativeLargeBright","negativeLargeDark","negativeSmallBright","negativeSmallDark","neutralLargeBright","neutralLargeDark","neutralSmallBright","neutralSmallDark") # eliminate annoying space in some level names
comps.grand.avg.allbins$size <- revalue(factor(comps.grand.avg.allbins$bin),c("all"="all","negativeLargeBright"="large","negativeLargeDark"="large","negativeSmallBright"="small","negativeSmallDark"="small","neutralLargeBright"="large","neutralLargeDark"="large","neutralSmallBright"="small","neutralSmallDark"="small")) # main effect of size
comps.grand.avg.allbins$cont <- revalue(factor(comps.grand.avg.allbins$bin),c("all"="all","negativeLargeBright"="bright","negativeLargeDark"="dark","negativeSmallBright"="bright","negativeSmallDark"="dark","neutralLargeBright"="bright","neutralLargeDark"="dark","neutralSmallBright"="bright","neutralSmallDark"="dark")) # main effect of contrast
comps.grand.avg.allbins$emo <- revalue(factor(comps.grand.avg.allbins$bin),c("all"="all","negativeLargeBright"="negative","negativeLargeDark"="negative","negativeSmallBright"="negative","negativeSmallDark"="negative","neutralLargeBright"="neutral","neutralLargeDark"="neutral","neutralSmallBright"="neutral","neutralSmallDark"="neutral")) # main effect of emotion
```

# P1

Amplitude extracted from the following electrode cluster:   
Iz, P9, P10, PO7, PO8, P7, O2, P6, P8, TP7, P5, PO4, TP8, P4, CP6.   

```{r P1_graph}
# plot P1
ggplot(subset(comps.grand.avg.allbins,component=="P1" & bin!="all"),aes(timepoint,amplitude)) + # basic plot
  geom_line(aes(color=bin),size=.8,alpha=.6) + # one line per bin
  stat_summary(fun.y=mean,geom="line",size=3) + # mean of all bins
  scale_colour_brewer(palette="Dark2") + # color palette
  labs(title="P1 (66 - 133 ms)",x="time (ms)",y=expression(paste("amplitude (",mu,"V)"))) + # title & axes labels
  scale_x_continuous(breaks=seq(-200,1000,50),limits=c(-100,250)) + # x-axis: tick marks
  scale_y_reverse(breaks=seq(-3,3,1),limits=c(3,-3)) + # y-axis: tick marks
  geom_vline(xintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # vertical reference line
  geom_hline(yintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # horizontal reference line
  annotate("rect",xmin=66,xmax=133,ymin=-1,ymax=2.5,alpha=.2) + # highlight time window used for analysis
  theme_pander(base_size=20,pc="white",lp=c(.5,.8)) # custom theme
```

Obvious latency differences across conditions will be explored in additional analyses.   

The collapsed localizer (black line) suggests that the time window for P1 amplitude extraction could be larger. We select 66 - 148 ms.

```{r P1_graph_shifted}
ggplot(subset(comps.grand.avg.allbins,component=="P1" & bin!="all"),aes(timepoint,amplitude)) + # basic plot
  geom_line(aes(color=bin),size=.8,alpha=.6) + # one line per bin
  stat_summary(fun.y=mean,geom="line",size=3) + # mean of all bins
  scale_colour_brewer(palette="Dark2") + # color palette
  labs(title="P1 (66 - 148 ms)",x="time (ms)",y=expression(paste("amplitude (",mu,"V)"))) + # title & axes labels
  scale_x_continuous(breaks=seq(-200,1000,50),limits=c(-100,250)) + # x-axis: tick marks
  scale_y_reverse(breaks=seq(-3,3,1),limits=c(3,-3)) + # y-axis: tick marks
  geom_vline(xintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # vertical reference line
  geom_hline(yintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # horizontal reference line
  annotate("rect",xmin=66,xmax=148,ymin=-1,ymax=2.5,alpha=.2) + # highlight time window used for analysis
  theme_pander(base_size=20,pc="white",lp=c(.5,.8)) # custom theme
```

```{r P1_topo}
# average across selected time points
P1.topos.grand.avg.allbins <- summarySEwithin(subset(grand.avg.allbins,timepoint >= 65 & timepoint <= 148),"amplitude",withinvars="electrode")
# electrode locations and amplitudes in the same data frame
P1.topos.grand.avg.allbins <- cbind(electrodeLocs,P1.topos.grand.avg.allbins[,"amplitude"])
names(P1.topos.grand.avg.allbins)[8] <- "amplitude" # change variable name

# plot topography
amplim <- c(-.5,.5) # min/max amplitude limit
contour.binwidth <- .1 # map contour

splineSmooth <- gam(amplitude~s(x,y,bs='ts'),data=P1.topos.grand.avg.allbins)
GAMtopo <- data.frame(expand.grid(x=seq(min(P1.topos.grand.avg.allbins$x)*2,
                                        max(P1.topos.grand.avg.allbins$x)*2,
                                        length=gridRes),
                                  y=seq(min(P1.topos.grand.avg.allbins$y)*2,
                                        max(P1.topos.grand.avg.allbins$y)*2,
                                        length=gridRes)))
GAMtopo$amplitude <-  predict(splineSmooth,GAMtopo,type="response")
GAMtopo$incircle <- (GAMtopo$x)^2+(GAMtopo$y)^2<.7^2
# plot topography
ggplot(GAMtopo[GAMtopo$incircle,],aes(x,y,fill=amplitude)) +
  geom_raster() +
  stat_contour(aes(z=amplitude),binwidth=contour.binwidth) +
  theme_topo() +
  scale_fill_gradientn(colours=jet.colors(10),
                       limits=amplim,
                       guide="colourbar",
                       oob=squish) +
  geom_path(data=maskRing,aes(x,y,z=NULL,fill=NULL),colour="white",size=6) +
  geom_point(data=P1.topos.grand.avg.allbins,
             aes(x,y,fill=NULL)) +
  geom_path(data=nose,aes(x,y,z=NULL,fill=NULL),size=1.5) +
  geom_path(data=headShape,aes(x,y,z=NULL,fill=NULL),size=1.5) +
  ggtitle("P1 (66 - 148 ms)") + 
  coord_quickmap()
```

# N1

Amplitude extracted from the following electrode cluster:   
CP5, TP7, CP6, TP8, P6, P8, P10, T7, P9, T8.   

```{r N1_graph}
# plot N1
ggplot(subset(comps.grand.avg.allbins,component=="N1" & bin!="all"),aes(timepoint,amplitude)) + # basic plot
  geom_line(aes(color=bin),size=.8,alpha=.6) + # one line per bin
  stat_summary(fun.y=mean,geom="line",size=3) + # mean of all bins
  scale_colour_brewer(palette="Dark2") + # color palette
  labs(title="N1 (164 - 238 ms)",x="time (ms)",y=expression(paste("amplitude (",mu,"V)"))) + # title & axes labels
  scale_x_continuous(breaks=seq(-200,1000,50),limits=c(-100,300)) + # x-axis: tick marks
  scale_y_reverse(breaks=seq(-3,3,1),limits=c(3,-3)) + # y-axis: tick marks
  geom_vline(xintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # vertical reference line
  geom_hline(yintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # horizontal reference line
  annotate("rect",xmin=164,xmax=242,ymin=-2,ymax=1,alpha=.2) + # highlight time window used for analysis
  theme_pander(base_size=20,pc="white",lp=c(.45,.8)) # custom theme
```

The collapsed localizer (black line) suggests that the time window for N1 amplitude extraction could be larger. We select 150 - 260 ms.

```{r N1_graph_large}
# plot N1
ggplot(subset(comps.grand.avg.allbins,component=="N1" & bin!="all"),aes(timepoint,amplitude)) + # basic plot
  geom_line(aes(color=bin),size=.8,alpha=.6) + # one line per bin
  stat_summary(fun.y=mean,geom="line",size=3) + # mean of all bins
  scale_colour_brewer(palette="Dark2") + # color palette
  labs(title="N1 (150 - 260 ms)",x="time (ms)",y=expression(paste("amplitude (",mu,"V)"))) + # title & axes labels
  scale_x_continuous(breaks=seq(-200,1000,50),limits=c(-100,300)) + # x-axis: tick marks
  scale_y_reverse(breaks=seq(-3,3,1),limits=c(3,-3)) + # y-axis: tick marks
  geom_vline(xintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # vertical reference line
  geom_hline(yintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # horizontal reference line
  annotate("rect",xmin=150,xmax=260,ymin=-2,ymax=1,alpha=.2) + # highlight time window used for analysis
  theme_pander(base_size=20,pc="white",lp=c(.45,.8)) # custom theme
```

```{r N1_topo}
# average across selected time points
N1.topos.grand.avg.allbins <- summarySEwithin(subset(grand.avg.allbins,timepoint >= 150 & timepoint <= 260),"amplitude",withinvars="electrode")
# electrode locations and amplitudes in the same data frame
N1.topos.grand.avg.allbins <- cbind(electrodeLocs,N1.topos.grand.avg.allbins[,"amplitude"])
names(N1.topos.grand.avg.allbins)[8] <- "amplitude" # change variable name

# plot topography
amplim <- c(-.5,.5) # min/max amplitude limit
contour.binwidth <- .1 # map contour

splineSmooth <- gam(amplitude~s(x,y,bs='ts'),data=N1.topos.grand.avg.allbins)
GAMtopo <- data.frame(expand.grid(x=seq(min(N1.topos.grand.avg.allbins$x)*2,
                                        max(N1.topos.grand.avg.allbins$x)*2,
                                        length=gridRes),
                                  y=seq(min(N1.topos.grand.avg.allbins$y)*2,
                                        max(N1.topos.grand.avg.allbins$y)*2,
                                        length=gridRes)))
GAMtopo$amplitude <-  predict(splineSmooth,GAMtopo,type="response")
GAMtopo$incircle <- (GAMtopo$x)^2+(GAMtopo$y)^2<.7^2
# plot topography
ggplot(GAMtopo[GAMtopo$incircle,],aes(x,y,fill=amplitude)) +
  geom_raster() +
  stat_contour(aes(z=amplitude),binwidth=contour.binwidth) +
  theme_topo() +
  scale_fill_gradientn(colours=jet.colors(10),
                       limits=amplim,
                       guide="colourbar",
                       oob=squish) +
  geom_path(data=maskRing,aes(x,y,z=NULL,fill=NULL),colour="white",size=6) +
  geom_point(data=N1.topos.grand.avg.allbins,
             aes(x,y,fill=NULL)) +
  geom_path(data=nose,aes(x,y,z=NULL,fill=NULL),size=1.5) +
  geom_path(data=headShape,aes(x,y,z=NULL,fill=NULL),size=1.5) +
  ggtitle("N1 (150 - 260 ms)") + 
  coord_quickmap()
```

# EPN

Amplitude extracted from the following electrode cluster:   
CP3, P3, P4, P1, P2, P6, PO4, CP6, CP4, PO3, Pz, CP5, POz, P8, PO8, CP1.   

```{r EPN_graph}
# plot EPN
ggplot(subset(comps.grand.avg.allbins,component=="EPN" & bin!="all"),aes(timepoint,amplitude)) + # basic plot
  geom_line(aes(color=bin),size=.8,alpha=.6) + # one line per bin
  stat_summary(fun.y=mean,geom="line",size=3) + # mean of all bins
  scale_colour_brewer(palette="Dark2") + # color palette
  labs(title="EPN (242 - 387 ms)",x="time (ms)",y=expression(paste("amplitude (",mu,"V)"))) + # title & axes labels
  scale_x_continuous(breaks=seq(-200,1000,50),limits=c(-100,600)) + # x-axis: tick marks
  scale_y_reverse(breaks=seq(-3,3,1),limits=c(3,-3)) + # y-axis: tick marks
  geom_vline(xintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # vertical reference line
  geom_hline(yintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # horizontal reference line
  annotate("rect",xmin=242,xmax=387,ymin=-.5,ymax=2,alpha=.2) + # highlight time window used for analysis
  theme_pander(base_size=20,pc="white",lp=c(.45,.8)) # custom theme
```

Observations:   
1. overall positive amplitude, but the EPN is better visualized as a difference wave (negative-neutral);   
2. the electrode cluster is more parietal than expected based on previous studies;   
3. the time window now slightly overlaps with the new N1 time window (by 260 - 242 = 18 ms).   

Address point 1: Plot EPN as a difference wave (negative minus neutral).

```{r EPN_graph_diffwave}
# plot EPN (negative minus neutral)
EPN.grand.avg.allbins <- summarySEwithin(subset(comps.grand.avg.allbins,component=="EPN" & bin!="all",select=-c(bin,size,cont)),measurevar="amplitude",withinvars=c("timepoint","emo"),idvar="component")
EPN.grand.avg.allbins$timepoint <- as.numeric(levels(EPN.grand.avg.allbins$timepoint))[EPN.grand.avg.allbins$timepoint] # re-convert time points to number
# plot
ggplot(EPN.grand.avg.allbins,aes(timepoint,amplitude)) + # basic plot
  geom_line(aes(color=emo),size=.8,alpha=.6) + # one line per emotion
  stat_summary(fun.y=diff,geom="line",size=3) + # mean of all bins
  scale_colour_brewer(palette="Dark2") + # color palette
  labs(title="EPN (242 - 387 ms), neg-neut",x="time (ms)",y=expression(paste("amplitude (",mu,"V)"))) + # title & axes labels
  scale_x_continuous(breaks=seq(-200,1000,50),limits=c(-100,600)) + # x-axis: tick marks
  scale_y_reverse(breaks=seq(-3,3,1),limits=c(2,-1)) + # y-axis: tick marks
  geom_vline(xintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # vertical reference line
  geom_hline(yintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # horizontal reference line
  annotate("rect",xmin=242,xmax=387,ymin=-.5,ymax=1.5,alpha=.2) + # highlight time window used for analysis
  theme_pander(base_size=20,pc="white",lp=c(.4,.3)) # custom theme
```

Address point 2: Use electrode cluster identified for N1 component (typically done in the literature):   
CP5, TP7, CP6, TP8, P6, P8, P10, T7, P9, T8.

```{r EPN_graph_diffwave_N1_electrodes}
# plot EPN using electrodes of N1
EPN.N1.grand.avg.allbins <- summarySEwithin(subset(comps.grand.avg.allbins,component=="N1" & bin!="all",select=-c(bin,size,cont)),measurevar="amplitude",withinvars=c("timepoint","emo"),idvar="component")
EPN.N1.grand.avg.allbins$timepoint <- as.numeric(levels(EPN.N1.grand.avg.allbins$timepoint))[EPN.N1.grand.avg.allbins$timepoint] # re-convert time points to number
# plot
ggplot(EPN.N1.grand.avg.allbins,aes(timepoint,amplitude)) + # basic plot
  geom_line(aes(color=emo),size=.8,alpha=.6) + # one line per emotion
  stat_summary(fun.y=diff,geom="line",size=3) + # mean of all bins
  scale_colour_brewer(palette="Dark2") + # color palette
  labs(title="EPN (242 - 387 ms), neg-neut, N1 electrodes",x="time (ms)",y=expression(paste("amplitude (",mu,"V)"))) + # title & axes labels
  scale_x_continuous(breaks=seq(-200,1000,50),limits=c(-100,600)) + # x-axis: tick marks
  scale_y_reverse(breaks=seq(-3,3,1),limits=c(1,-1.2)) + # y-axis: tick marks
  geom_vline(xintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # vertical reference line
  geom_hline(yintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # horizontal reference line
  annotate("rect",xmin=242,xmax=387,ymin=-.5,ymax=1,alpha=.2) + # highlight time window used for analysis
  theme_pander(base_size=20,pc="white",lp=c(.25,.8)) # custom theme
```

Address point 3: now the time window naturally changes to 280 - 402 ms.

```{r EPN_graph_diffwave_N1_electrodes_shifted}
# plot
ggplot(EPN.N1.grand.avg.allbins,aes(timepoint,amplitude)) + # basic plot
  geom_line(aes(color=emo),size=.8,alpha=.6) + # one line per emotion
  stat_summary(fun.y=diff,geom="line",size=3) + # mean of all bins
  scale_colour_brewer(palette="Dark2") + # color palette
  labs(title="EPN (280 - 402 ms), neg-neut N1 electrodes",x="time (ms)",y=expression(paste("amplitude (",mu,"V)"))) + # title & axes labels
  scale_x_continuous(breaks=seq(-200,1000,50),limits=c(-100,600)) + # x-axis: tick marks
  scale_y_reverse(breaks=seq(-3,3,1),limits=c(1,-1.2)) + # y-axis: tick marks
  geom_vline(xintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # vertical reference line
  geom_hline(yintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # horizontal reference line
  annotate("rect",xmin=280,xmax=402,ymin=-.5,ymax=1,alpha=.2) + # highlight time window used for analysis
  theme_pander(base_size=20,pc="white",lp=c(.25,.8)) # custom theme
```

```{r EPN_topo}
# average across selected time points
EPN.topos.grand.avg.allbins <- summarySEwithin(subset(grand.avg.allbins,timepoint >= 280 & timepoint <= 402),"amplitude",withinvars="electrode")
# electrode locations and amplitudes in the same data frame
EPN.topos.grand.avg.allbins <- cbind(electrodeLocs,EPN.topos.grand.avg.allbins[,"amplitude"])
names(EPN.topos.grand.avg.allbins)[8] <- "amplitude" # change variable name

# plot topography
amplim <- c(-.5,.5) # min/max amplitude limit
contour.binwidth <- .1 # map contour

splineSmooth <- gam(amplitude~s(x,y,bs='ts'),data=EPN.topos.grand.avg.allbins)
GAMtopo <- data.frame(expand.grid(x=seq(min(EPN.topos.grand.avg.allbins$x)*2,
                                        max(EPN.topos.grand.avg.allbins$x)*2,
                                        length=gridRes),
                                  y=seq(min(EPN.topos.grand.avg.allbins$y)*2,
                                        max(EPN.topos.grand.avg.allbins$y)*2,
                                        length=gridRes)))
GAMtopo$amplitude <-  predict(splineSmooth,GAMtopo,type="response")
GAMtopo$incircle <- (GAMtopo$x)^2+(GAMtopo$y)^2<.7^2
# plot topography
ggplot(GAMtopo[GAMtopo$incircle,],aes(x,y,fill=amplitude)) +
  geom_raster() +
  stat_contour(aes(z=amplitude),binwidth=contour.binwidth) +
  theme_topo() +
  scale_fill_gradientn(colours=jet.colors(10),
                       limits=amplim,
                       guide="colourbar",
                       oob=squish) +
  geom_path(data=maskRing,aes(x,y,z=NULL,fill=NULL),colour="white",size=6) +
  geom_point(data=EPN.topos.grand.avg.allbins,
             aes(x,y,fill=NULL)) +
  geom_path(data=nose,aes(x,y,z=NULL,fill=NULL),size=1.5) +
  geom_path(data=headShape,aes(x,y,z=NULL,fill=NULL),size=1.5) +
  ggtitle("EPN (280 - 402 ms)") + 
  coord_quickmap()
```

# LPP

Amplitude extracted from the following electrode cluster:   
P5, P3, PO3, P1, Pz, POz, P2, P4, PO4, P6, PO8, CP6, P8, CP5, TP8, CP3, CP4, CP1, CP2, CPz, C5, C3, C4.   

```{r LPP_graph}
# plot LPP
ggplot(subset(comps.grand.avg.allbins,component=="LPP" & bin!="all"),aes(timepoint,amplitude)) + # basic plot
  geom_line(aes(color=bin),size=.8,alpha=.6) + # one line per bin
  stat_summary(fun.y=mean,geom="line",size=3) + # mean of all bins
  scale_colour_brewer(palette="Dark2") + # color palette
  labs(title="LPP (402 - 684 ms)",x="time (ms)",y=expression(paste("amplitude (",mu,"V)"))) + # title & axes labels
  scale_x_continuous(breaks=seq(-200,1000,50)) + # x-axis: tick marks
  scale_y_reverse(breaks=seq(-3,3,1),limits=c(3,-3)) + # y-axis: tick marks
  geom_vline(xintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # vertical reference line
  geom_hline(yintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # horizontal reference line
  annotate("rect",xmin=402,xmax=684,ymin=-1,ymax=2,alpha=.2) + # highlight time window used for analysis
  theme_pander(base_size=20,pc="white",lp=c(.8,.8)) # custom theme
```

The collapsed localizer (black line) suggests that the time window for LPP amplitude extraction could be larger. We select 402 - 1000 ms.

```{r LPP_graph_large}
# plot LPP
ggplot(subset(comps.grand.avg.allbins,component=="LPP" & bin!="all"),aes(timepoint,amplitude)) + # basic plot
  geom_line(aes(color=bin),size=.8,alpha=.6) + # one line per bin
  stat_summary(fun.y=mean,geom="line",size=3) + # mean of all bins
  scale_colour_brewer(palette="Dark2") + # color palette
  labs(title="LPP (402 - 1000 ms)",x="time (ms)",y=expression(paste("amplitude (",mu,"V)"))) + # title & axes labels
  scale_x_continuous(breaks=seq(-200,1000,50)) + # x-axis: tick marks
  scale_y_reverse(breaks=seq(-3,3,1),limits=c(3,-3)) + # y-axis: tick marks
  geom_vline(xintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # vertical reference line
  geom_hline(yintercept=0,linetype="dashed",colour="black",size=.8,alpha=.8) + # horizontal reference line
  annotate("rect",xmin=402,xmax=1000,ymin=-1,ymax=2,alpha=.2) + # highlight time window used for analysis
  theme_pander(base_size=20,pc="white",lp=c(.8,.8)) # custom theme
```

```{r LPP_topo}
# average across selected time points
LPP.topos.grand.avg.allbins <- summarySEwithin(subset(grand.avg.allbins,timepoint >= 402 & timepoint <= 1000),"amplitude",withinvars="electrode")
# electrode locations and amplitudes in the same data frame
LPP.topos.grand.avg.allbins <- cbind(electrodeLocs,LPP.topos.grand.avg.allbins[,"amplitude"])
names(LPP.topos.grand.avg.allbins)[8] <- "amplitude" # change variable name

# plot topography
amplim <- c(-.5,.5) # min/max amplitude limit
contour.binwidth <- .1 # map contour

splineSmooth <- gam(amplitude~s(x,y,bs='ts'),data=LPP.topos.grand.avg.allbins)
GAMtopo <- data.frame(expand.grid(x=seq(min(LPP.topos.grand.avg.allbins$x)*2,
                                        max(LPP.topos.grand.avg.allbins$x)*2,
                                        length=gridRes),
                                  y=seq(min(LPP.topos.grand.avg.allbins$y)*2,
                                        max(LPP.topos.grand.avg.allbins$y)*2,
                                        length=gridRes)))
GAMtopo$amplitude <-  predict(splineSmooth,GAMtopo,type="response")
GAMtopo$incircle <- (GAMtopo$x)^2+(GAMtopo$y)^2<.7^2
# plot topography
ggplot(GAMtopo[GAMtopo$incircle,],aes(x,y,fill=amplitude)) +
  geom_raster() +
  stat_contour(aes(z=amplitude),binwidth=contour.binwidth) +
  theme_topo() +
  scale_fill_gradientn(colours=jet.colors(10),
                       limits=amplim,
                       guide="colourbar",
                       oob=squish) +
  geom_path(data=maskRing,aes(x,y,z=NULL,fill=NULL),colour="white",size=6) +
  geom_point(data=LPP.topos.grand.avg.allbins,
             aes(x,y,fill=NULL)) +
  geom_path(data=nose,aes(x,y,z=NULL,fill=NULL),size=1.5) +
  geom_path(data=headShape,aes(x,y,z=NULL,fill=NULL),size=1.5) +
  ggtitle("LPP (402 - 1000 ms)") + 
  coord_quickmap()
```

<center> <h1>*ANALYSIS*</h1> </center>

```{r main_data}
data.EEG.trial <- read.csv(paste0(expphase,"_vis_trialEEG.csv"),header=TRUE) # load data
data.EEG.trial$size <- revalue(factor(data.EEG.trial$bin),c("1"="large","2"="small","3"="large","4"="small","5"="large","6"="small","7"="large","8"="small")) # main effect of size
data.EEG.trial$cont <- revalue(factor(data.EEG.trial$bin),c("1"="dark","2"="dark","3"="bright","4"="bright","5"="dark","6"="dark","7"="bright","8"="bright")) # main effect of contrast
data.EEG.trial$emo <- revalue(factor(data.EEG.trial$bin),c("1"="negative","2"="negative","3"="negative","4"="negative","5"="neutral","6"="neutral","7"="neutral","8"="neutral")) # main effect of emotion
data.EEG.trial$bin <- revalue(factor(data.EEG.trial$bin),c("1"="negativeLargeDark","2"="negativeSmallDark","3"="negativeLargeBright","4"="negativeSmallBright","5"="neutralLargeDark","6"="neutralSmallDark","7"="neutralLargeBright","8"="neutralSmallBright")) # recode bin variable

# add words for each trial
data.EEG.trialWords <- read.csv(paste0(expphase,"_words.csv"),header=TRUE) # load data
data.EEG.trial$word <- data.EEG.trialWords[,"word"] # include words in EEG data frame
rm(data.EEG.trialWords) # delete word data

data.EEG.trial <- within(data.EEG.trial,size<-relevel(size,ref="large")) # reference: large size
data.EEG.trial <- within(data.EEG.trial,cont<-relevel(cont,ref="dark")) # reference: high contrast
data.EEG.trial <- within(data.EEG.trial,emo<-relevel(emo,ref="neutral")) # reference: neutral emotion
```

We will calculate and compare the Bayes Factor of different linear mixed-effects models. The random factors are participants and the individual word per trial. Their variance is set as nuisance.   

We will compare (against the null model) the following models:   

1. main effects of size and emotion   
2. interactive effects of size and emotion   
3. main effects of contrast and emotion   
4. interactive effects of contrast and emotion   
5. main effects of size, contrast, and emotion   
6. interactive effects of size, contrast, and emotion   

We will then compare the best competing models to understand which one should be preferred overall.   
   
Gilles asked whether we can identify the component even in the single-trial data (see graphs).

```{r main_P1_ERPimage}
# # load data
# data.ERPimg.P1 <- read.csv(paste0(expphase,"_ERPimage_P1.csv"),header=TRUE,check.names=FALSE) # P1
# data.ERPimg.N1 <- read.csv(paste0(expphase,"_ERPimage_N1.csv"),header=TRUE,check.names=FALSE) # N1
# data.ERPimg.EPN <- read.csv(paste0(expphase,"_ERPimage_EPN.csv"),header=TRUE,check.names=FALSE) # EPN
# data.ERPimg.LPP <- read.csv(paste0(expphase,"_ERPimage_LPP.csv"),header=TRUE,check.names=FALSE) # LPP
# 
# data.ERPimg <- cbind(data.ERPimg.P1[,c("participant","bin","mean")],data.ERPimg.N1[,"mean"],data.ERPimg.EPN[,"mean"],data.ERPimg.LPP[,"mean"]) # merge components
# data.ERPimg$bin <- revalue(factor(data.ERPimg$bin),c("1"="negativeLargeDark","2"="negativeSmallDark","3"="negativeLargeBright","4"="negativeSmallBright","5"="neutralLargeDark","6"="neutralSmallDark","7"="neutralLargeBright","8"="neutralSmallBright")) # recode bin variable
# names(data.ERPimg) <- c("participant","bin","P1","N1","EPN","LPP") # change variable names
# # plots
# for(issj in levels(data.EEG.trial$participant)) { # loop through participants
#   for(icond in levels(data.EEG.trial$bin)) { # loop through conditions
#     
#     temp.data <- subset(data.ERPimg,participant==issj & bin==icond) # subset data
#     temp.data <- melt(temp.data,id.vars=c("participant","bin"),variable.name="component",value.name="amplitude") # transform to long format
#     temp.data$trials <- rep(1:length(which(temp.data$component=="P1")),length(unique(temp.data$component))) # add variable with trial counts
#     # plot
#     ggplot(temp.data,aes(component,trials,fill=amplitude)) +
#       geom_tile() +
#       ggtitle(paste0(issj,", ",icond)) + 
#       scale_fill_viridis(name=expression(paste(mu,"V")),limits=c(-3,3),oob=squish,option="magma") +
#       theme_classic(base_size=16) # classic theme
#     
#     ggsave(filename=paste0("ERPimage_",issj,"_",icond,".jpg"),path=paste0(wd,"graphs/"),width=8,height=8,units="in",dpi=600) # save to .jpg
#   } 
# }
```

It is obviously difficult to find patterns, but it seems that at least P1/N1 generally elicit positive/negative amplitudes in each trial.

# P1

```{r main_P1_models}
# summarize data
summary.P1 <- summarySEwithin(data.EEG.trial,"P1",withinvars=c("size","cont","emo"),idvar="participant")
kable(summary.P1)

niter <- 10000 # number of MCMC iterations
scaling.factor <- c(.5,.707,1) # scaling factors of JZS prior: narrow, medium, wide

compare.P1.BF <- matrix(NA,6,length(scaling.factor)) # preallocate matrix with all BF10
compare.P1.perc.err <- matrix(NA,6,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effects of size and emotion
  # P1.sizeplusemo.BF <- lmBF(P1~size+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(P1.sizeplusemo.BF,file=paste0("P1_sizeplusemo_BF_",scaling.factor[k],".rds")) # save model
  P1.sizeplusemo.BF <- readRDS(paste0("P1_sizeplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of size and emotion
  # P1.sizebyemo.BF <- lmBF(P1~size:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(P1.sizebyemo.BF,file=paste0("P1_sizebyemo_BF_",scaling.factor[k],".rds")) # save model
  P1.sizebyemo.BF <- readRDS(paste0("P1_sizebyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### main effects of contrast and emotion
  # P1.contplusemo.BF <- lmBF(P1~cont+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(P1.contplusemo.BF,file=paste0("P1_contplusemo_BF_",scaling.factor[k],".rds")) # save model
  P1.contplusemo.BF <- readRDS(paste0("P1_contplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of contrast and emotion
  # P1.contbyemo.BF <- lmBF(P1~cont:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(P1.contbyemo.BF,file=paste0("P1_contbyemo_BF_",scaling.factor[k],".rds")) # save model
  P1.contbyemo.BF <- readRDS(paste0("P1_contbyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### main effects of size, contrast, and emotion
  # P1.sizepluscontplusemo.BF <- lmBF(P1~size+cont+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(P1.sizepluscontplusemo.BF,file=paste0("P1_sizepluscontplusemo_BF_",scaling.factor[k],".rds")) # save model
  P1.sizepluscontplusemo.BF <- readRDS(paste0("P1_sizepluscontplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of size, contrast, and emotion
  # P1.sizebycontbyemo.BF <- lmBF(P1~size:cont:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(P1.sizebycontbyemo.BF,file=paste0("P1_sizebycontbyemo_BF_",scaling.factor[k],".rds")) # save model
  P1.sizebycontbyemo.BF <- readRDS(paste0("P1_sizebycontbyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.P1.BF[,k] <- c(exp(1)^P1.sizeplusemo.BF@bayesFactor$bf[1],exp(1)^P1.sizebyemo.BF@bayesFactor$bf[1],exp(1)^P1.contplusemo.BF@bayesFactor$bf[1],exp(1)^P1.contbyemo.BF@bayesFactor$bf[1],exp(1)^P1.sizepluscontplusemo.BF@bayesFactor$bf[1],exp(1)^P1.sizebycontbyemo.BF@bayesFactor$bf[1])
  # percentage of error
  compare.P1.perc.err[,k] <- c(P1.sizeplusemo.BF@bayesFactor$error[1]*100,P1.sizebyemo.BF@bayesFactor$error[1]*100,P1.contplusemo.BF@bayesFactor$error[1]*100,P1.contbyemo.BF@bayesFactor$error[1]*100,P1.sizepluscontplusemo.BF@bayesFactor$error[1]*100,P1.sizebycontbyemo.BF@bayesFactor$error[1]*100)
}
# summary
compare.P1 <- data.frame("model"=c("size + emo","size x emo","contr + emo","cont x emo","size + cont + emo","size x cont x emo"),
                         "nar"=compare.P1.BF[,1],"nar.p.err"=round(compare.P1.perc.err[,1],digits=3),
                         "med"=compare.P1.BF[,2],"med.p.err"=round(compare.P1.perc.err[,2],digits=3),
                         "wid"=compare.P1.BF[,3],"wid.p.err"=round(compare.P1.perc.err[,3],digits=3))
compare.P1 <- compare.P1[order(compare.P1$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.P1)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the model `r ifelse(compare.P1[1,4]<1,"null",as.character(compare.P1[1,1]))` ought to be preferred.   
The best model (`r as.character(compare.P1[1,1])`) explains the observed data `r ifelse(compare.P1[1,4]>1,compare.P1[1,4]/compare.P1[2,4],1/compare.P1[1,4])` times better than the second best model (`r as.character(compare.P1[2,1])`).   

## Paired comparisons

```{r main_P1_posthoc_size}
# summarize data
summary.P1.size <- summarySEwithin(data.EEG.trial,"P1",withinvars="size",idvar="participant")
kable(summary.P1.size)

# P1 graph
pirateplot(formula=P1~size, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="size", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-15,25), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.P1.size.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.P1.size.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of size 
  # P1.size.BF <- lmBF(P1~size,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(P1.size.BF,file=paste0("P1_size_BF_",scaling.factor[k],".rds")) # save model
  P1.size.BF <- readRDS(paste0("P1_size_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.P1.size.BF[,k] <- exp(1)^P1.size.BF@bayesFactor$bf[1]
  # percentage of error
  compare.P1.size.perc.err[,k] <- P1.size.BF@bayesFactor$error[1]*100
}

# summary
compare.P1.size <- data.frame("model"="size",
                              "nar"=compare.P1.size.BF[,1],"nar.p.err"=round(compare.P1.size.perc.err[,1],digits=3),
                              "med"=compare.P1.size.BF[,2],"med.p.err"=round(compare.P1.size.perc.err[,2],digits=3),
                              "wid"=compare.P1.size.BF[,3],"wid.p.err"=round(compare.P1.size.perc.err[,3],digits=3))
compare.P1.size <- compare.P1.size[order(compare.P1.size$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.P1.size)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.P1.size[1,4]<1,"null",as.character(compare.P1.size[1,1]))` model explains the observed data `r ifelse(compare.P1.size[1,4]>1,compare.P1.size[1,4],1/compare.P1.size[1,4])` times better than the `r ifelse(compare.P1.size[1,4]>1,"null",as.character(compare.P1.size[1,1]))` model.

```{r main_P1_posthoc_cont}
# summarize data
summary.P1.cont <- summarySEwithin(data.EEG.trial,"P1",withinvars="cont",idvar="participant")
kable(summary.P1.cont)

# P1 graph
pirateplot(formula=P1~cont, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="contrast", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-15,25), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.P1.cont.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.P1.cont.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of contrast 
  # P1.cont.BF <- lmBF(P1~cont,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(P1.cont.BF,file=paste0("P1_cont_BF_",scaling.factor[k],".rds")) # save model
  P1.cont.BF <- readRDS(paste0("P1_cont_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.P1.cont.BF[,k] <- exp(1)^P1.cont.BF@bayesFactor$bf[1]
  # percentage of error
  compare.P1.cont.perc.err[,k] <- P1.cont.BF@bayesFactor$error[1]*100
}

# summary
compare.P1.cont <- data.frame("model"="contrast",
                              "nar"=compare.P1.cont.BF[,1],"nar.p.err"=round(compare.P1.cont.perc.err[,1],digits=3),
                              "med"=compare.P1.cont.BF[,2],"med.p.err"=round(compare.P1.cont.perc.err[,2],digits=3),
                              "wid"=compare.P1.cont.BF[,3],"wid.p.err"=round(compare.P1.cont.perc.err[,3],digits=3))
compare.P1.cont <- compare.P1.cont[order(compare.P1.cont$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.P1.cont)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.P1.cont[1,4]<1,"null",as.character(compare.P1.cont[1,1]))` model explains the observed data `r ifelse(compare.P1.cont[1,4]>1,compare.P1.cont[1,4],1/compare.P1.cont[1,4])` times better than the `r ifelse(compare.P1.cont[1,4]>1,"null",as.character(compare.P1.cont[1,1]))` model.

```{r main_P1_posthoc_emo}
# summarize data
summary.P1.emo <- summarySEwithin(data.EEG.trial,"P1",withinvars="emo",idvar="participant")
kable(summary.P1.emo)

# P1 graph
pirateplot(formula=P1~emo, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="emotion", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-15,25), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.P1.emo.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.P1.emo.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of emotion 
  # P1.emo.BF <- lmBF(P1~emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(P1.emo.BF,file=paste0("P1_emo_BF_",scaling.factor[k],".rds")) # save model
  P1.emo.BF <- readRDS(paste0("P1_emo_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.P1.emo.BF[,k] <- exp(1)^P1.emo.BF@bayesFactor$bf[1]
  # percentage of error
  compare.P1.emo.perc.err[,k] <- P1.emo.BF@bayesFactor$error[1]*100
}

# summary
compare.P1.emo <- data.frame("model"="emotion",
                             "nar"=compare.P1.emo.BF[,1],"nar.p.err"=round(compare.P1.emo.perc.err[,1],digits=3),
                             "med"=compare.P1.emo.BF[,2],"med.p.err"=round(compare.P1.emo.perc.err[,2],digits=3),
                             "wid"=compare.P1.emo.BF[,3],"wid.p.err"=round(compare.P1.emo.perc.err[,3],digits=3))
compare.P1.emo <- compare.P1.emo[order(compare.P1.emo$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.P1.emo)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.P1.emo[1,4]<1,"null",as.character(compare.P1.emo[1,1]))` model explains the observed data `r ifelse(compare.P1.emo[1,4]>1,compare.P1.emo[1,4],1/compare.P1.emo[1,4])` times better than the `r ifelse(compare.P1.emo[1,4]>1,"null",as.character(compare.P1.emo[1,1]))` model.   

Note that the size + cont + emo model wins over all the other ones because we do not build the size + cont model, because we are only interested in models that include the effect of emotion (even if it doesn't play a role).

# N1

```{r main_N1_models}
# summarize data
summary.N1 <- summarySEwithin(data.EEG.trial,"N1",withinvars=c("size","cont","emo"),idvar="participant")
kable(summary.N1)

compare.N1.BF <- matrix(NA,6,length(scaling.factor)) # preallocate matrix with all BF10
compare.N1.perc.err <- matrix(NA,6,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effects of size and emotion
  # N1.sizeplusemo.BF <- lmBF(N1~size+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(N1.sizeplusemo.BF,file=paste0("N1_sizeplusemo_BF_",scaling.factor[k],".rds")) # save model
  N1.sizeplusemo.BF <- readRDS(paste0("N1_sizeplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of size and emotion
  # N1.sizebyemo.BF <- lmBF(N1~size:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(N1.sizebyemo.BF,file=paste0("N1_sizebyemo_BF_",scaling.factor[k],".rds")) # save model
  N1.sizebyemo.BF <- readRDS(paste0("N1_sizebyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### main effects of contrast and emotion
  # N1.contplusemo.BF <- lmBF(N1~cont+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(N1.contplusemo.BF,file=paste0("N1_contplusemo_BF_",scaling.factor[k],".rds")) # save model
  N1.contplusemo.BF <- readRDS(paste0("N1_contplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of contrast and emotion
  # N1.contbyemo.BF <- lmBF(N1~cont:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(N1.contbyemo.BF,file=paste0("N1_contbyemo_BF_",scaling.factor[k],".rds")) # save model
  N1.contbyemo.BF <- readRDS(paste0("N1_contbyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### main effects of size, contrast, and emotion
  # N1.sizepluscontplusemo.BF <- lmBF(N1~size+cont+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(N1.sizepluscontplusemo.BF,file=paste0("N1_sizepluscontplusemo_BF_",scaling.factor[k],".rds")) # save model
  N1.sizepluscontplusemo.BF <- readRDS(paste0("N1_sizepluscontplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of size, contrast, and emotion
  # N1.sizebycontbyemo.BF <- lmBF(N1~size:cont:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(N1.sizebycontbyemo.BF,file=paste0("N1_sizebycontbyemo_BF_",scaling.factor[k],".rds")) # save model
  N1.sizebycontbyemo.BF <- readRDS(paste0("N1_sizebycontbyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.N1.BF[,k] <- c(exp(1)^N1.sizeplusemo.BF@bayesFactor$bf[1],exp(1)^N1.sizebyemo.BF@bayesFactor$bf[1],exp(1)^N1.contplusemo.BF@bayesFactor$bf[1],exp(1)^N1.contbyemo.BF@bayesFactor$bf[1],exp(1)^N1.sizepluscontplusemo.BF@bayesFactor$bf[1],exp(1)^N1.sizebycontbyemo.BF@bayesFactor$bf[1])
  # percentage of error
  compare.N1.perc.err[,k] <- c(N1.sizeplusemo.BF@bayesFactor$error[1]*100,N1.sizebyemo.BF@bayesFactor$error[1]*100,N1.contplusemo.BF@bayesFactor$error[1]*100,N1.contbyemo.BF@bayesFactor$error[1]*100,N1.sizepluscontplusemo.BF@bayesFactor$error[1]*100,N1.sizebycontbyemo.BF@bayesFactor$error[1]*100)
}
# summary
compare.N1 <- data.frame("model"=c("size + emo","size x emo","contr + emo","cont x emo","size + cont + emo","size x cont x emo"),
                         "nar"=compare.N1.BF[,1],"nar.p.err"=round(compare.N1.perc.err[,1],digits=3),
                         "med"=compare.N1.BF[,2],"med.p.err"=round(compare.N1.perc.err[,2],digits=3),
                         "wid"=compare.N1.BF[,3],"wid.p.err"=round(compare.N1.perc.err[,3],digits=3))
compare.N1 <- compare.N1[order(compare.N1$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.N1)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the model `r ifelse(compare.N1[1,4]<1,"null",as.character(compare.N1[1,1]))` ought to be preferred.   
The best model (`r as.character(compare.N1[1,1])`) explains the observed data `r ifelse(compare.N1[1,4]>1,compare.N1[1,4]/compare.N1[2,4],1/compare.N1[1,4])` times better than the second best model (`r as.character(compare.N1[2,1])`).   

## Paired comparisons

```{r main_N1_posthoc_size}
# summarize data
summary.N1.size <- summarySEwithin(data.EEG.trial,"N1",withinvars="size",idvar="participant")
kable(summary.N1.size)

# N1 graph
pirateplot(formula=N1~size, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="size", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-15,20), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.N1.size.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.N1.size.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of size 
  # N1.size.BF <- lmBF(N1~size,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(N1.size.BF,file=paste0("N1_size_BF_",scaling.factor[k],".rds")) # save model
  N1.size.BF <- readRDS(paste0("N1_size_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.N1.size.BF[,k] <- exp(1)^N1.size.BF@bayesFactor$bf[1]
  # percentage of error
  compare.N1.size.perc.err[,k] <- N1.size.BF@bayesFactor$error[1]*100
}

# summary
compare.N1.size <- data.frame("model"="size",
                              "nar"=compare.N1.size.BF[,1],"nar.p.err"=round(compare.N1.size.perc.err[,1],digits=3),
                              "med"=compare.N1.size.BF[,2],"med.p.err"=round(compare.N1.size.perc.err[,2],digits=3),
                              "wid"=compare.N1.size.BF[,3],"wid.p.err"=round(compare.N1.size.perc.err[,3],digits=3))
compare.N1.size <- compare.N1.size[order(compare.N1.size$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.N1.size)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.N1.size[1,4]<1,"null",as.character(compare.N1.size[1,1]))` model explains the observed data `r ifelse(compare.N1.size[1,4]>1,compare.N1.size[1,4],1/compare.N1.size[1,4])` times better than the `r ifelse(compare.N1.size[1,4]>1,"null",as.character(compare.N1.size[1,1]))` model.

```{r main_N1_posthoc_cont}
# summarize data
summary.N1.cont <- summarySEwithin(data.EEG.trial,"N1",withinvars="cont",idvar="participant")
kable(summary.N1.cont)

# N1 graph
pirateplot(formula=N1~cont, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="contrast", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-15,20), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.N1.cont.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.N1.cont.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of contrast 
  # N1.cont.BF <- lmBF(N1~cont,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(N1.cont.BF,file=paste0("N1_cont_BF_",scaling.factor[k],".rds")) # save model
  N1.cont.BF <- readRDS(paste0("N1_cont_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.N1.cont.BF[,k] <- exp(1)^N1.cont.BF@bayesFactor$bf[1]
  # percentage of error
  compare.N1.cont.perc.err[,k] <- N1.cont.BF@bayesFactor$error[1]*100
}

# summary
compare.N1.cont <- data.frame("model"="contrast",
                              "nar"=compare.N1.cont.BF[,1],"nar.p.err"=round(compare.N1.cont.perc.err[,1],digits=3),
                              "med"=compare.N1.cont.BF[,2],"med.p.err"=round(compare.N1.cont.perc.err[,2],digits=3),
                              "wid"=compare.N1.cont.BF[,3],"wid.p.err"=round(compare.N1.cont.perc.err[,3],digits=3))
compare.N1.cont <- compare.N1.cont[order(compare.N1.cont$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.N1.cont)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.N1.cont[1,4]<1,"null",as.character(compare.N1.cont[1,1]))` model explains the observed data `r ifelse(compare.N1.cont[1,4]>1,compare.N1.cont[1,4],1/compare.N1.cont[1,4])` times better than the `r ifelse(compare.N1.cont[1,4]>1,"null",as.character(compare.N1.cont[1,1]))` model.

```{r main_N1_posthoc_emo}
# summarize data
summary.N1.emo <- summarySEwithin(data.EEG.trial,"N1",withinvars="emo",idvar="participant")
kable(summary.N1.emo)

# N1 graph
pirateplot(formula=N1~emo, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="emotion", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-15,20), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.N1.emo.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.N1.emo.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of emotion 
  # N1.emo.BF <- lmBF(N1~emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(N1.emo.BF,file=paste0("N1_emo_BF_",scaling.factor[k],".rds")) # save model
  N1.emo.BF <- readRDS(paste0("N1_emo_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.N1.emo.BF[,k] <- exp(1)^N1.emo.BF@bayesFactor$bf[1]
  # percentage of error
  compare.N1.emo.perc.err[,k] <- N1.emo.BF@bayesFactor$error[1]*100
}

# summary
compare.N1.emo <- data.frame("model"="emotion",
                             "nar"=compare.N1.emo.BF[,1],"nar.p.err"=round(compare.N1.emo.perc.err[,1],digits=3),
                             "med"=compare.N1.emo.BF[,2],"med.p.err"=round(compare.N1.emo.perc.err[,2],digits=3),
                             "wid"=compare.N1.emo.BF[,3],"wid.p.err"=round(compare.N1.emo.perc.err[,3],digits=3))
compare.N1.emo <- compare.N1.emo[order(compare.N1.emo$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.N1.emo)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.N1.emo[1,4]<1,"null",as.character(compare.N1.emo[1,1]))` model explains the observed data `r ifelse(compare.N1.emo[1,4]>1,compare.N1.emo[1,4],1/compare.N1.emo[1,4])` times better than the `r ifelse(compare.N1.emo[1,4]>1,"null",as.character(compare.N1.emo[1,1]))` model.   

# EPN

```{r main_EPN_models}
# summarize data
summary.EPN <- summarySEwithin(data.EEG.trial,"EPN",withinvars=c("size","cont","emo"),idvar="participant")
kable(summary.EPN)

compare.EPN.BF <- matrix(NA,6,length(scaling.factor)) # preallocate matrix with all BF10
compare.EPN.perc.err <- matrix(NA,6,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effects of size and emotion
  # EPN.sizeplusemo.BF <- lmBF(EPN~size+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(EPN.sizeplusemo.BF,file=paste0("EPN_sizeplusemo_BF_",scaling.factor[k],".rds")) # save model
  EPN.sizeplusemo.BF <- readRDS(paste0("EPN_sizeplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of size and emotion
  # EPN.sizebyemo.BF <- lmBF(EPN~size:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(EPN.sizebyemo.BF,file=paste0("EPN_sizebyemo_BF_",scaling.factor[k],".rds")) # save model
  EPN.sizebyemo.BF <- readRDS(paste0("EPN_sizebyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### main effects of contrast and emotion
  # EPN.contplusemo.BF <- lmBF(EPN~cont+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(EPN.contplusemo.BF,file=paste0("EPN_contplusemo_BF_",scaling.factor[k],".rds")) # save model
  EPN.contplusemo.BF <- readRDS(paste0("EPN_contplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of contrast and emotion
  # EPN.contbyemo.BF <- lmBF(EPN~cont:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(EPN.contbyemo.BF,file=paste0("EPN_contbyemo_BF_",scaling.factor[k],".rds")) # save model
  EPN.contbyemo.BF <- readRDS(paste0("EPN_contbyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### main effects of size, contrast, and emotion
  # EPN.sizepluscontplusemo.BF <- lmBF(EPN~size+cont+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(EPN.sizepluscontplusemo.BF,file=paste0("EPN_sizepluscontplusemo_BF_",scaling.factor[k],".rds")) # save model
  EPN.sizepluscontplusemo.BF <- readRDS(paste0("EPN_sizepluscontplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of size, contrast, and emotion
  # EPN.sizebycontbyemo.BF <- lmBF(EPN~size:cont:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(EPN.sizebycontbyemo.BF,file=paste0("EPN_sizebycontbyemo_BF_",scaling.factor[k],".rds")) # save model
  EPN.sizebycontbyemo.BF <- readRDS(paste0("EPN_sizebycontbyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.EPN.BF[,k] <- c(exp(1)^EPN.sizeplusemo.BF@bayesFactor$bf[1],exp(1)^EPN.sizebyemo.BF@bayesFactor$bf[1],exp(1)^EPN.contplusemo.BF@bayesFactor$bf[1],exp(1)^EPN.contbyemo.BF@bayesFactor$bf[1],exp(1)^EPN.sizepluscontplusemo.BF@bayesFactor$bf[1],exp(1)^EPN.sizebycontbyemo.BF@bayesFactor$bf[1])
  # percentage of error
  compare.EPN.perc.err[,k] <- c(EPN.sizeplusemo.BF@bayesFactor$error[1]*100,EPN.sizebyemo.BF@bayesFactor$error[1]*100,EPN.contplusemo.BF@bayesFactor$error[1]*100,EPN.contbyemo.BF@bayesFactor$error[1]*100,EPN.sizepluscontplusemo.BF@bayesFactor$error[1]*100,EPN.sizebycontbyemo.BF@bayesFactor$error[1]*100)
}
# summary
compare.EPN <- data.frame("model"=c("size + emo","size x emo","contr + emo","cont x emo","size + cont + emo","size x cont x emo"),
                          "nar"=compare.EPN.BF[,1],"nar.p.err"=round(compare.EPN.perc.err[,1],digits=3),
                          "med"=compare.EPN.BF[,2],"med.p.err"=round(compare.EPN.perc.err[,2],digits=3),
                          "wid"=compare.EPN.BF[,3],"wid.p.err"=round(compare.EPN.perc.err[,3],digits=3))
compare.EPN <- compare.EPN[order(compare.EPN$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.EPN)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the model `r ifelse(compare.EPN[1,4]<1,"null",as.character(compare.EPN[1,1]))` ought to be preferred.   
The best model (`r as.character(compare.EPN[1,1])`) explains the observed data `r ifelse(compare.EPN[1,4]>1,compare.EPN[1,4]/compare.EPN[2,4],1/compare.EPN[1,4])` times better than the second best model (`r as.character(compare.EPN[2,1])`).   

## Paired comparisons

```{r main_EPN_posthoc_size}
# summarize data
summary.EPN.size <- summarySEwithin(data.EEG.trial,"EPN",withinvars="size",idvar="participant")
kable(summary.EPN.size)

# EPN graph
pirateplot(formula=EPN~size, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="size", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-15,20), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.EPN.size.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.EPN.size.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of size 
  # EPN.size.BF <- lmBF(EPN~size,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(EPN.size.BF,file=paste0("EPN_size_BF_",scaling.factor[k],".rds")) # save model
  EPN.size.BF <- readRDS(paste0("EPN_size_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.EPN.size.BF[,k] <- exp(1)^EPN.size.BF@bayesFactor$bf[1]
  # percentage of error
  compare.EPN.size.perc.err[,k] <- EPN.size.BF@bayesFactor$error[1]*100
}

# summary
compare.EPN.size <- data.frame("model"="size",
                               "nar"=compare.EPN.size.BF[,1],"nar.p.err"=round(compare.EPN.size.perc.err[,1],digits=3),
                               "med"=compare.EPN.size.BF[,2],"med.p.err"=round(compare.EPN.size.perc.err[,2],digits=3),
                               "wid"=compare.EPN.size.BF[,3],"wid.p.err"=round(compare.EPN.size.perc.err[,3],digits=3))
compare.EPN.size <- compare.EPN.size[order(compare.EPN.size$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.EPN.size)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.EPN.size[1,4]<1,"null",as.character(compare.EPN.size[1,1]))` model explains the observed data `r ifelse(compare.EPN.size[1,4]>1,compare.EPN.size[1,4],1/compare.EPN.size[1,4])` times better than the `r ifelse(compare.EPN.size[1,4]>1,"null",as.character(compare.EPN.size[1,1]))` model.

```{r main_EPN_posthoc_cont}
# summarize data
summary.EPN.cont <- summarySEwithin(data.EEG.trial,"EPN",withinvars="cont",idvar="participant")
kable(summary.EPN.cont)

# EPN graph
pirateplot(formula=EPN~cont, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="contrast", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-15,20), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.EPN.cont.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.EPN.cont.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of contrast 
  # EPN.cont.BF <- lmBF(EPN~cont,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(EPN.cont.BF,file=paste0("EPN_cont_BF_",scaling.factor[k],".rds")) # save model
  EPN.cont.BF <- readRDS(paste0("EPN_cont_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.EPN.cont.BF[,k] <- exp(1)^EPN.cont.BF@bayesFactor$bf[1]
  # percentage of error
  compare.EPN.cont.perc.err[,k] <- EPN.cont.BF@bayesFactor$error[1]*100
}

# summary
compare.EPN.cont <- data.frame("model"="contrast",
                               "nar"=compare.EPN.cont.BF[,1],"nar.p.err"=round(compare.EPN.cont.perc.err[,1],digits=3),
                               "med"=compare.EPN.cont.BF[,2],"med.p.err"=round(compare.EPN.cont.perc.err[,2],digits=3),
                               "wid"=compare.EPN.cont.BF[,3],"wid.p.err"=round(compare.EPN.cont.perc.err[,3],digits=3))
compare.EPN.cont <- compare.EPN.cont[order(compare.EPN.cont$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.EPN.cont)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.EPN.cont[1,4]<1,"null",as.character(compare.EPN.cont[1,1]))` model explains the observed data `r ifelse(compare.EPN.cont[1,4]>1,compare.EPN.cont[1,4],1/compare.EPN.cont[1,4])` times better than the `r ifelse(compare.EPN.cont[1,4]>1,"null",as.character(compare.EPN.cont[1,1]))` model.

```{r main_EPN_posthoc_emo}
# summarize data
summary.EPN.emo <- summarySEwithin(data.EEG.trial,"EPN",withinvars="emo",idvar="participant")
kable(summary.EPN.emo)

# EPN graph
pirateplot(formula=EPN~emo, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="emotion", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-15,20), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.EPN.emo.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.EPN.emo.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of emotion 
  # EPN.emo.BF <- lmBF(EPN~emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(EPN.emo.BF,file=paste0("EPN_emo_BF_",scaling.factor[k],".rds")) # save model
  EPN.emo.BF <- readRDS(paste0("EPN_emo_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.EPN.emo.BF[,k] <- exp(1)^EPN.emo.BF@bayesFactor$bf[1]
  # percentage of error
  compare.EPN.emo.perc.err[,k] <- EPN.emo.BF@bayesFactor$error[1]*100
}

# summary
compare.EPN.emo <- data.frame("model"="emotion",
                              "nar"=compare.EPN.emo.BF[,1],"nar.p.err"=round(compare.EPN.emo.perc.err[,1],digits=3),
                              "med"=compare.EPN.emo.BF[,2],"med.p.err"=round(compare.EPN.emo.perc.err[,2],digits=3),
                              "wid"=compare.EPN.emo.BF[,3],"wid.p.err"=round(compare.EPN.emo.perc.err[,3],digits=3))
compare.EPN.emo <- compare.EPN.emo[order(compare.EPN.emo$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.EPN.emo)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.EPN.emo[1,4]<1,"null",as.character(compare.EPN.emo[1,1]))` model explains the observed data `r ifelse(compare.EPN.emo[1,4]>1,compare.EPN.emo[1,4],1/compare.EPN.emo[1,4])` times better than the `r ifelse(compare.EPN.emo[1,4]>1,"null",as.character(compare.EPN.emo[1,1]))` model.   

# LPP

```{r main_LPP_models}
# summarize data
summary.LPP <- summarySEwithin(data.EEG.trial,"LPP",withinvars=c("size","cont","emo"),idvar="participant")
kable(summary.LPP)

compare.LPP.BF <- matrix(NA,6,length(scaling.factor)) # preallocate matrix with all BF10
compare.LPP.perc.err <- matrix(NA,6,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effects of size and emotion
  # LPP.sizeplusemo.BF <- lmBF(LPP~size+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(LPP.sizeplusemo.BF,file=paste0("LPP_sizeplusemo_BF_",scaling.factor[k],".rds")) # save model
  LPP.sizeplusemo.BF <- readRDS(paste0("LPP_sizeplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of size and emotion
  # LPP.sizebyemo.BF <- lmBF(LPP~size:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(LPP.sizebyemo.BF,file=paste0("LPP_sizebyemo_BF_",scaling.factor[k],".rds")) # save model
  LPP.sizebyemo.BF <- readRDS(paste0("LPP_sizebyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### main effects of contrast and emotion
  # LPP.contplusemo.BF <- lmBF(LPP~cont+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(LPP.contplusemo.BF,file=paste0("LPP_contplusemo_BF_",scaling.factor[k],".rds")) # save model
  LPP.contplusemo.BF <- readRDS(paste0("LPP_contplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of contrast and emotion
  # LPP.contbyemo.BF <- lmBF(LPP~cont:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(LPP.contbyemo.BF,file=paste0("LPP_contbyemo_BF_",scaling.factor[k],".rds")) # save model
  LPP.contbyemo.BF <- readRDS(paste0("LPP_contbyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### main effects of size, contrast, and emotion
  # LPP.sizepluscontplusemo.BF <- lmBF(LPP~size+cont+emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(LPP.sizepluscontplusemo.BF,file=paste0("LPP_sizepluscontplusemo_BF_",scaling.factor[k],".rds")) # save model
  LPP.sizepluscontplusemo.BF <- readRDS(paste0("LPP_sizepluscontplusemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### interactive effects of size, contrast, and emotion
  # LPP.sizebycontbyemo.BF <- lmBF(LPP~size:cont:emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(LPP.sizebycontbyemo.BF,file=paste0("LPP_sizebycontbyemo_BF_",scaling.factor[k],".rds")) # save model
  LPP.sizebycontbyemo.BF <- readRDS(paste0("LPP_sizebycontbyemo_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.LPP.BF[,k] <- c(exp(1)^LPP.sizeplusemo.BF@bayesFactor$bf[1],exp(1)^LPP.sizebyemo.BF@bayesFactor$bf[1],exp(1)^LPP.contplusemo.BF@bayesFactor$bf[1],exp(1)^LPP.contbyemo.BF@bayesFactor$bf[1],exp(1)^LPP.sizepluscontplusemo.BF@bayesFactor$bf[1],exp(1)^LPP.sizebycontbyemo.BF@bayesFactor$bf[1])
  # percentage of error
  compare.LPP.perc.err[,k] <- c(LPP.sizeplusemo.BF@bayesFactor$error[1]*100,LPP.sizebyemo.BF@bayesFactor$error[1]*100,LPP.contplusemo.BF@bayesFactor$error[1]*100,LPP.contbyemo.BF@bayesFactor$error[1]*100,LPP.sizepluscontplusemo.BF@bayesFactor$error[1]*100,LPP.sizebycontbyemo.BF@bayesFactor$error[1]*100)
}
# summary
compare.LPP <- data.frame("model"=c("size + emo","size x emo","contr + emo","cont x emo","size + cont + emo","size x cont x emo"),
                          "nar"=compare.LPP.BF[,1],"nar.p.err"=round(compare.LPP.perc.err[,1],digits=3),
                          "med"=compare.LPP.BF[,2],"med.p.err"=round(compare.LPP.perc.err[,2],digits=3),
                          "wid"=compare.LPP.BF[,3],"wid.p.err"=round(compare.LPP.perc.err[,3],digits=3))
compare.LPP <- compare.LPP[order(compare.LPP$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.LPP)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the model `r ifelse(compare.LPP[1,4]<1,"null",as.character(compare.LPP[1,1]))` ought to be preferred.   
The best model (`r as.character(compare.LPP[1,1])`) explains the observed data `r ifelse(compare.LPP[1,4]>1,compare.LPP[1,4]/compare.LPP[2,4],1/compare.LPP[1,4])` times better than the second best model (`r as.character(compare.LPP[2,1])`).   

## Paired comparisons

```{r main_LPP_posthoc_size}
# summarize data
summary.LPP.size <- summarySEwithin(data.EEG.trial,"LPP",withinvars="size",idvar="participant")
kable(summary.LPP.size)

# LPP graph
pirateplot(formula=LPP~size, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="size", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-10,10), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.LPP.size.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.LPP.size.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of size 
  # LPP.size.BF <- lmBF(LPP~size,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(LPP.size.BF,file=paste0("LPP_size_BF_",scaling.factor[k],".rds")) # save model
  LPP.size.BF <- readRDS(paste0("LPP_size_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.LPP.size.BF[,k] <- exp(1)^LPP.size.BF@bayesFactor$bf[1]
  # percentage of error
  compare.LPP.size.perc.err[,k] <- LPP.size.BF@bayesFactor$error[1]*100
}

# summary
compare.LPP.size <- data.frame("model"="size",
                               "nar"=compare.LPP.size.BF[,1],"nar.p.err"=round(compare.LPP.size.perc.err[,1],digits=3),
                               "med"=compare.LPP.size.BF[,2],"med.p.err"=round(compare.LPP.size.perc.err[,2],digits=3),
                               "wid"=compare.LPP.size.BF[,3],"wid.p.err"=round(compare.LPP.size.perc.err[,3],digits=3))
compare.LPP.size <- compare.LPP.size[order(compare.LPP.size$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.LPP.size)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.LPP.size[1,4]<1,"null",as.character(compare.LPP.size[1,1]))` model explains the observed data `r ifelse(compare.LPP.size[1,4]>1,compare.LPP.size[1,4],1/compare.LPP.size[1,4])` times better than the `r ifelse(compare.LPP.size[1,4]>1,"null",as.character(compare.LPP.size[1,1]))` model.

```{r main_LPP_posthoc_cont}
# summarize data
summary.LPP.cont <- summarySEwithin(data.EEG.trial,"LPP",withinvars="cont",idvar="participant")
kable(summary.LPP.cont)

# LPP graph
pirateplot(formula=LPP~cont, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="contrast", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-10,10), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.LPP.cont.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.LPP.cont.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of contrast 
  # LPP.cont.BF <- lmBF(LPP~cont,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(LPP.cont.BF,file=paste0("LPP_cont_BF_",scaling.factor[k],".rds")) # save model
  LPP.cont.BF <- readRDS(paste0("LPP_cont_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.LPP.cont.BF[,k] <- exp(1)^LPP.cont.BF@bayesFactor$bf[1]
  # percentage of error
  compare.LPP.cont.perc.err[,k] <- LPP.cont.BF@bayesFactor$error[1]*100
}

# summary
compare.LPP.cont <- data.frame("model"="contrast",
                               "nar"=compare.LPP.cont.BF[,1],"nar.p.err"=round(compare.LPP.cont.perc.err[,1],digits=3),
                               "med"=compare.LPP.cont.BF[,2],"med.p.err"=round(compare.LPP.cont.perc.err[,2],digits=3),
                               "wid"=compare.LPP.cont.BF[,3],"wid.p.err"=round(compare.LPP.cont.perc.err[,3],digits=3))
compare.LPP.cont <- compare.LPP.cont[order(compare.LPP.cont$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.LPP.cont)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.LPP.cont[1,4]<1,"null",as.character(compare.LPP.cont[1,1]))` model explains the observed data `r ifelse(compare.LPP.cont[1,4]>1,compare.LPP.cont[1,4],1/compare.LPP.cont[1,4])` times better than the `r ifelse(compare.LPP.cont[1,4]>1,"null",as.character(compare.LPP.cont[1,1]))` model.

```{r main_LPP_posthoc_emo}
# summarize data
summary.LPP.emo <- summarySEwithin(data.EEG.trial,"LPP",withinvars="emo",idvar="participant")
kable(summary.LPP.emo)

# LPP graph
pirateplot(formula=LPP~emo, # dependent~independent variables
           data=data.EEG.trial, # data frame
           main="emotion", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(-10,10), # y-axis: limits
           ylab=expression(paste("amplitude (",mu,"V)")), # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           inf.within="participant", # ID variable in within-subject designs
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="xmen") # color palette [see piratepal(palette="all")]

compare.LPP.emo.BF <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all BF10
compare.LPP.emo.perc.err <- matrix(NA,1,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of emotion 
  # LPP.emo.BF <- lmBF(LPP~emo,data.EEG.trial,whichRandom=c("participant","word"),rscaleFixed=scaling.factor[k],rscaleRandom="nuisance",rscaleCont="medium",iterations=niter,posterior=FALSE)
  # saveRDS(LPP.emo.BF,file=paste0("LPP_emo_BF_",scaling.factor[k],".rds")) # save model
  LPP.emo.BF <- readRDS(paste0("LPP_emo_BF_",scaling.factor[k],".rds")) # load model
  
  ### model comparison
  # BFs
  compare.LPP.emo.BF[,k] <- exp(1)^LPP.emo.BF@bayesFactor$bf[1]
  # percentage of error
  compare.LPP.emo.perc.err[,k] <- LPP.emo.BF@bayesFactor$error[1]*100
}

# summary
compare.LPP.emo <- data.frame("model"="emotion",
                              "nar"=compare.LPP.emo.BF[,1],"nar.p.err"=round(compare.LPP.emo.perc.err[,1],digits=3),
                              "med"=compare.LPP.emo.BF[,2],"med.p.err"=round(compare.LPP.emo.perc.err[,2],digits=3),
                              "wid"=compare.LPP.emo.BF[,3],"wid.p.err"=round(compare.LPP.emo.perc.err[,3],digits=3))
compare.LPP.emo <- compare.LPP.emo[order(compare.LPP.emo$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(compare.LPP.emo)
```

When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the `r ifelse(compare.LPP.emo[1,4]<1,"null",as.character(compare.LPP.emo[1,1]))` model explains the observed data `r ifelse(compare.LPP.emo[1,4]>1,compare.LPP.emo[1,4],1/compare.LPP.emo[1,4])` times better than the `r ifelse(compare.LPP.emo[1,4]>1,"null",as.character(compare.LPP.emo[1,1]))` model.   


