---
title: "<center> <h1>***SIZECONTEMO - ANALYSIS BEHAVIORAL DATA***</h1> </center>"
author: '[Antonio Schettino](https://osf.io/zbv65/ "Antonio Schettino")'
date: '`r Sys.Date()`'
output:
  html_document:
    theme: united
    highlight: tango
    code_folding: hide
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

Analysis of behavioral data.

```{r setup_environment,echo=FALSE,warning=FALSE,message=FALSE}
# setup work environment
# dev.off() # clear plots (if no plots are present, comment it out or it will throw an error)
cat("\014") # clear console
rm(list=ls()) # clear environment
set.seed(9001) # specify seed for RNG and ensure reproducible results (it's over 9000!)

expname <- "SizeContEmo" # experiment name
expphase <- "main" # experiment phase
wd <- paste0("E:/Experiments/",expname,"/analysis/behavior/",expphase,"/") # work directory
setwd(wd) # set work directory

# use pacman to install and load relevant packages
pacman::p_load("knitr", # dynamic report generation
               "tidyverse", # install the following packages: ggplot2, tibble, tidyr, readr, purrr, dplyr
               "Rmisc", # for advanced summary functions
               "yarrr", # amazing graphs
               "BayesFactor") # calculate Bayes factors

# setup report output
options(width=120,scipen=999,digits=3) # change output width (for better printing), disable scientific notation (default: scipen=0), constrain output to 4 decimals
opts_chunk$set(warning=FALSE,message=FALSE,fig.width=10,fig.height=6) # for each chunk, display the output but not the R code (echo=FALSE), no package warnings (message=FALSE), no package messages (message=FALSE), width and height of all figures

niter <- 100000 # number of MCMC iterations for calculation of Bayes Factors
scaling.factor <- c(.5,.707,1) # scaling factors of JZS prior: narrow, medium, wide
```

```{r data}
data.behav <- read.csv("data_behavior.csv",header=TRUE) # load data
data.behav <- data.behav %>% # data frame
  mutate(cond=paste(size,cont,sep="_"), # create variable with merged conditions
    size=relevel(size,ref="large"), # re-reference size to large
         cont=relevel(cont,ref="dark")) # re-reference contrast to dark
```

We will calculate and compare the Bayes Factor of different linear mixed-effects models. The random factors are participants and words, and their variance set as nuisance.   
   
We will compare (against the null model) the following models:   
   
1. main effect of size
2. main effect of contrast
3. main effects of size and contrast
4. interactive effects of size and contrast
   
We will then compare the best competing models to understand which one should be preferred overall.   

# RT

```{r rt_pirateplot}
# summarize data
summary.rt <- data.behav %>% # data frame
  summarySEwithin(data=.,measurevar="RT",withinvars=c("size","cont"),idvar="ssj",na.rm=TRUE)
kable(summary.rt,digits=2)

# pirateplot
# better visualization with averages across trials
summary.rt.plot <- summarySEwithin(data.behav,measurevar="RT",withinvars=c("ssj","size","cont"),idvar="word",na.rm=TRUE)
pirateplot(formula=RT~cont+size, # dependent~independent variables
           data=summary.rt.plot, # data frame
           main="RTs", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(400,850), # y-axis: limits
           ylab="ms", # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="southpark") # color palette [see piratepal(palette="all")]
```  

```{r rt_models}
compare.rt.BF <- matrix(NA,4,length(scaling.factor)) # preallocate matrix with all BFs
compare.rt.perc.err <- matrix(NA,4,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of size
  rt.BF.size <- lmBF(RT~size, # formula
                     data.behav[complete.cases(data.behav),], # data (omit missing values)
                     whichRandom=c("ssj","word"), # random effects
                     rscaleFixed=scaling.factor[k], # scaling factor of prior on effect size
                     rscaleRandom="nuisance", # prior scale for standardized random effects
                     rscaleCont="medium", # prior scale for standardized slopes
                     iterations=niter) # number of MCMC iterations
                     
  ### main effect of contrast
  rt.BF.cont <- lmBF(RT~cont, # formula
                     data.behav[complete.cases(data.behav),], # data (omit missing values)
                     whichRandom=c("ssj","word"), # random effects
                     rscaleFixed=scaling.factor[k], # scaling factor of prior on effect size
                     rscaleRandom="nuisance", # prior scale for standardized random effects
                     rscaleCont="medium", # prior scale for standardized slopes
                     iterations=niter) # number of MCMC iterations

  ### main effects of size and contrast
  rt.BF.sizepluscont <- lmBF(RT~size+cont, # formula
                             data.behav[complete.cases(data.behav),], # data (omit missing values)
                             whichRandom=c("ssj","word"), # random effects
                             rscaleFixed=scaling.factor[k], # scaling factor of prior on effect size
                             rscaleRandom="nuisance", # prior scale for standardized random effects
                             rscaleCont="medium", # prior scale for standardized slopes
                             iterations=niter) # number of MCMC iterations
  
  ### full model: main effects of size and contrast + their interaction
  rt.BF.full <- lmBF(RT~size*cont, # formula
                     data.behav[complete.cases(data.behav),], # data (omit missing values)
                     whichRandom=c("ssj","word"), # random effects
                     rscaleFixed=scaling.factor[k], # scaling factor of prior on effect size
                     rscaleRandom="nuisance", # prior scale for standardized random effects
                     rscaleCont="medium", # prior scale for standardized slopes
                     iterations=niter) # number of MCMC iterations
  
  ### model comparison
  # BFs
  compare.rt.BF[,k] <- c(exp(rt.BF.size@bayesFactor$bf[1]),
                         exp(rt.BF.cont@bayesFactor$bf[1]),
                         exp(rt.BF.sizepluscont@bayesFactor$bf[1]),
                         exp(rt.BF.full@bayesFactor$bf[1]))
  # percentage of error
  compare.rt.perc.err[,k] <- c(rt.BF.size@bayesFactor$error[1]*100,
                               rt.BF.cont@bayesFactor$error[1]*100,
                               rt.BF.sizepluscont@bayesFactor$error[1]*100,
                               rt.BF.full@bayesFactor$error[1]*100)
}
# summary confirmatory analysis
compare.rt <- data.frame("model"=c("size","contr","size + contr","size x cont"),
                         "nar"=compare.rt.BF[,1],"nar.p.err"=compare.rt.perc.err[,1],
                         "med"=compare.rt.BF[,2],"med.p.err"=compare.rt.perc.err[,2],
                         "wid"=compare.rt.BF[,3],"wid.p.err"=compare.rt.perc.err[,3])
compare.rt <- compare.rt[order(compare.rt$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(format(compare.rt,scientific=TRUE),digits=2)
```
   
When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the model `r ifelse(compare.rt[1,4]<1,"null",as.character(compare.rt[1,1]))` ought to be preferred.   
The best model (`r ifelse(compare.rt[1,4]>1,as.character(compare.rt[1,1]),"null")`) explains the observed data `r ifelse(compare.rt[1,4]>1,compare.rt[1,4]/compare.rt[2,4],1/compare.rt[1,4])` times better than the second best model (`r ifelse(compare.rt[1,4]>1,as.character(compare.rt[2,1]),as.character(compare.rt[1,1]))`).   

## Paired comparisons

```{r rt_pirateplot}
data.rt <- data.behav %>% # data frame
  select(-resp) %>% # eliminate unused variables
  filter(!is.na(RT)) %>% # eliminate NAs (misses)
  summarySEwithin(data=.,measurevar="RT",withinvars=c("ssj","cond")) %>% # summary
  split(.$cond) # split according to condition

compare.rt.posthocBF <- matrix(NA,4,length(scaling.factor)) # preallocate matrix with all BF10
compare.rt.posthocBF.perc.err <- matrix(NA,4,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  # large, dark vs. bright
  rt.posthocBF.large.darkVSbright <- ttestBF(data.rt$large_dark$RT, # first condition
                                             data.rt$large_bright$RT, # second condition
                                             mu=0, # null hypothesis (mean difference=0)
                                             paired=TRUE, # paired sample test
                                             iterations=niter, # number of MCMC iterations
                                             rscale=scaling.factor[k]) # scaling factor
  
  # small, dark vs. bright
  rt.posthocBF.small.darkVSbright <- ttestBF(data.rt$small_dark$RT, # first condition
                                             data.rt$small_bright$RT, # second condition
                                             mu=0, # null hypothesis (mean difference=0)
                                             paired=TRUE, # paired sample test
                                             iterations=niter, # number of MCMC iterations
                                             rscale=scaling.factor[k]) # scaling factor
  
  # dark, large vs. small
  rt.posthocBF.dark.largeVSsmall <- ttestBF(data.rt$large_dark$RT, # first condition
                                            data.rt$small_dark$RT, # second condition
                                            mu=0, # null hypothesis (mean difference=0)
                                            paired=TRUE, # paired sample test
                                            iterations=niter, # number of MCMC iterations
                                            rscale=scaling.factor[k]) # scaling factor
  
  # bright, large vs. small
  rt.posthocBF.bright.largeVSsmall <- ttestBF(data.rt$large_bright$RT, # first condition
                                              data.rt$small_bright$RT, # second condition
                                              mu=0, # null hypothesis (mean difference=0)
                                              paired=TRUE, # paired sample test
                                              iterations=niter, # number of MCMC iterations
                                              rscale=scaling.factor[k]) # scaling factor
 
  ### model comparison
  compare.rt.posthocBF[,k] <- c(exp(rt.posthocBF.large.darkVSbright@bayesFactor$bf[1]),
                                exp(rt.posthocBF.small.darkVSbright@bayesFactor$bf[1]),
                                exp(rt.posthocBF.dark.largeVSsmall@bayesFactor$bf[1]),
                                exp(rt.posthocBF.bright.largeVSsmall@bayesFactor$bf[1]))
  
  # percentage of error
  compare.rt.posthocBF.perc.err[,k] <- c(rt.posthocBF.large.darkVSbright@bayesFactor$error[1]*100,
                                         rt.posthocBF.small.darkVSbright@bayesFactor$error[1]*100,
                                         rt.posthocBF.dark.largeVSsmall@bayesFactor$error[1]*100,
                                         rt.posthocBF.bright.largeVSsmall@bayesFactor$error[1]*100)
}
# summary
compare.rt.posthocBF <- data.frame("comparison"=c("large.darkVSbright","small.darkVSbright","dark.largeVSsmall","bright.largeVSsmall"),
                                   "nar"=compare.rt.posthocBF[,1],"nar.p.err"=compare.rt.posthocBF.perc.err[,1],
                                   "med"=compare.rt.posthocBF[,2],"med.p.err"=compare.rt.posthocBF.perc.err[,2],
                                   "wid"=compare.rt.posthocBF[,3],"wid.p.err"=compare.rt.posthocBF.perc.err[,3])
compare.rt.posthocBF <- compare.rt.posthocBF[order(compare.rt.posthocBF$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(format(compare.rt.posthocBF,scientific=TRUE),digits=2)
```

###########################################################
# FROM HERE
Comment on posthoc results 
###########################################################







```{r acc_temp}
# calculate proportion of correct responses per participant and condition
data.acc <- ddply(data.behav,.(ssj,size,cont),summarize,
        freqs=length(which(resp=="hit"))/length(resp))

# summarize data
summary.acc <- data.acc %>% # data frame
  summarySEwithin(data=.,measurevar="freqs",withinvars=c("size","cont"),idvar="ssj",na.rm=TRUE)
kable(summary.acc,digits=2)

# pirateplot
pirateplot(formula=freqs~cont+size, # dependent~independent variables
           data=data.acc, # data frame
           main="accuracy", # main title
           xlim=NULL, # x-axis: limits
           xlab="",  # x-axis: label
           ylim=c(.8,1), # y-axis: limits
           ylab="proportion of correct responses", # y-axis: label
           inf.method="hdi", # type of inference: 95% Bayesian Highest Density Interval (HDI)
           hdi.iter=5000, # number of iterations for 95% HDI
           cap.beans=TRUE, # max and min values of bean densities are capped at the limits found in the data
           pal="southpark") # color palette [see piratepal(palette="all")]
```

```{r acc_models}
compare.acc.BF <- matrix(NA,4,length(scaling.factor)) # preallocate matrix with all BFs
compare.acc.perc.err <- matrix(NA,4,length(scaling.factor)) # preallocate matrix with all % errors

for(k in 1:length(scaling.factor)) { # loop through scaling factors
  
  ### main effect of size
  acc.BF.size <- lmBF(freqs~size, # formula
                     data.acc, # data
                     whichRandom="ssj", # random effects
                     rscaleFixed=scaling.factor[k], # scaling factor of prior on effect size
                     rscaleRandom="nuisance", # prior scale for standardized random effects
                     rscaleCont="medium", # prior scale for standardized slopes
                     iterations=niter) # number of MCMC iterations
                     
  ### main effect of contrast
  acc.BF.cont <- lmBF(freqs~cont, # formula
                     data.acc, # data
                     whichRandom="ssj", # random effects
                     rscaleFixed=scaling.factor[k], # scaling factor of prior on effect size
                     rscaleRandom="nuisance", # prior scale for standardized random effects
                     rscaleCont="medium", # prior scale for standardized slopes
                     iterations=niter) # number of MCMC iterations

  ### main effects of size and contrast
  acc.BF.sizepluscont <- lmBF(freqs~size+cont, # formula
                             data.acc, # data
                             whichRandom="ssj", # random effects
                             rscaleFixed=scaling.factor[k], # scaling factor of prior on effect size
                             rscaleRandom="nuisance", # prior scale for standardized random effects
                             rscaleCont="medium", # prior scale for standardized slopes
                             iterations=niter) # number of MCMC iterations
  
  ### full model: main effects of size and contrast + their interaction
  acc.BF.full <- lmBF(freqs~size*cont, # formula
                     data.acc, # data
                     whichRandom="ssj", # random effects
                     rscaleFixed=scaling.factor[k], # scaling factor of prior on effect size
                     rscaleRandom="nuisance", # prior scale for standardized random effects
                     rscaleCont="medium", # prior scale for standardized slopes
                     iterations=niter) # number of MCMC iterations
  
  ### model comparison
  # BFs
  compare.acc.BF[,k] <- c(exp(acc.BF.size@bayesFactor$bf[1]),
                         exp(acc.BF.cont@bayesFactor$bf[1]),
                         exp(acc.BF.sizepluscont@bayesFactor$bf[1]),
                         exp(acc.BF.full@bayesFactor$bf[1]))
  # percentage of error
  compare.acc.perc.err[,k] <- c(acc.BF.size@bayesFactor$error[1]*100,
                               acc.BF.cont@bayesFactor$error[1]*100,
                               acc.BF.sizepluscont@bayesFactor$error[1]*100,
                               acc.BF.full@bayesFactor$error[1]*100)
}
# summary confirmatory analysis
compare.acc <- data.frame("model"=c("size","contr","size + contr","size x cont"),
                         "nar"=compare.acc.BF[,1],"nar.p.err"=compare.acc.perc.err[,1],
                         "med"=compare.acc.BF[,2],"med.p.err"=compare.acc.perc.err[,2],
                         "wid"=compare.acc.BF[,3],"wid.p.err"=compare.acc.perc.err[,3])
compare.acc <- compare.acc[order(compare.acc$med,decreasing=TRUE),] # sort according to medium scaling factor (in descending order)
kable(format(compare.acc,scientific=TRUE),digits=2)
```
   
When using a JZS prior with scaling factor r=`r scaling.factor[2]` placed on standardized effect sizes, the model `r ifelse(compare.rt[1,4]<1,"null",as.character(compare.rt[1,1]))` ought to be preferred.   
The best model (`r ifelse(compare.rt[1,4]>1,as.character(compare.rt[1,1]),"null")`) explains the observed data `r format(ifelse(compare.rt[1,4]>1,compare.rt[1,4]/compare.rt[2,4],1/compare.rt[1,4]),scientific=TRUE)` times better than the second best model (`r ifelse(compare.rt[1,4]>1,as.character(compare.rt[2,1]),as.character(compare.rt[1,1]))`).   

## Paired comparisons




